{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0198d517-aef2-40ab-9295-8e01cf1d2fad",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5aef79c-e376-4f34-b95b-76a9cfa49c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets \n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision.transforms as transforms \n",
    "from torch.utils.tensorboard import SummaryWriter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf6a7b-f7e2-45f1-9bf5-8f3d50da7a52",
   "metadata": {},
   "source": [
    "## Main Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7618a0a7-e06b-480d-81c5-0ac7945313b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new class that inherits from torch.nn.Module, which is the base class for all PyTorch neural networks.\n",
    "class Discriminator(nn.Module):\n",
    "    # in_features, which is the size of the input vector\n",
    "    def __init__(self,in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "                 nn.Linear(in_features,128),\n",
    "                 nn.LeakyReLU(0.01),\n",
    "                 nn.Linear(128,1),\n",
    "            # binary classifier that takes an image as input and outputs a probability (0 = fake, 1 = real).\n",
    "                 nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "133c8d81-edf2-431d-b83c-2c7e6654855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # noise_dim: This is the size of the input noise vector\n",
    "    # image_dim: This is the size of the output image\n",
    "    def __init__(self,noise_dim,image_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(noise_dim,256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256,image_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward ( self,x):\n",
    "        return self.gen(x)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08400787-d01e-45b7-bdd3-f94717470abf",
   "metadata": {},
   "source": [
    "## Hyperparameters & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db847683-2711-42e6-8da7-56a16f4253dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "lr = 3e-4\n",
    "noise_dim = 64 \n",
    "image_dim = 28*28*1 \n",
    "batch_size = 32 \n",
    "num_epochs = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b78f0286-3393-4887-ab0c-fe3645427d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(noise_dim,image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size,noise_dim)).to(device)\n",
    "\n",
    "# prepare your image dataset (e.g., MNIST or CIFAR) for training.\n",
    "transforms = transforms.Compose(\n",
    "    [   # Converts a PIL image or NumPy array to a PyTorch tensor.\n",
    "        transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5,),(0.5,)), ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3363febd-25ac-4dce-b96b-b7c9ddf0ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root=\"dataset/\" , transform=transforms , download = True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(),lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(),lr=lr)\n",
    "# The binary cross-entropy loss function\n",
    "criterion = nn.BCELoss()\n",
    "# Initializes TensorBoard writer for logging fake and real image statistics during training.\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742a66b-90ae-4747-9235-f66ed8a69c9e",
   "metadata": {},
   "source": [
    "## Train Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc05439f-b857-4ae1-8434-5b1c67711840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/1875                       Loss D: 0.6053, loss G: 0.7135\n",
      "Epoch [1/50] Batch 0/1875                       Loss D: 0.0038, loss G: 5.2382\n",
      "Epoch [2/50] Batch 0/1875                       Loss D: 0.0004, loss G: 7.9914\n",
      "Epoch [3/50] Batch 0/1875                       Loss D: 0.0002, loss G: 8.8522\n",
      "Epoch [4/50] Batch 0/1875                       Loss D: 0.0001, loss G: 9.8630\n",
      "Epoch [5/50] Batch 0/1875                       Loss D: 0.0000, loss G: 11.2355\n",
      "Epoch [6/50] Batch 0/1875                       Loss D: 0.0000, loss G: 12.2036\n",
      "Epoch [7/50] Batch 0/1875                       Loss D: 0.0000, loss G: 13.4619\n",
      "Epoch [8/50] Batch 0/1875                       Loss D: 0.0000, loss G: 14.7443\n",
      "Epoch [9/50] Batch 0/1875                       Loss D: 0.0000, loss G: 16.7441\n",
      "Epoch [10/50] Batch 0/1875                       Loss D: 0.0000, loss G: 17.5778\n",
      "Epoch [11/50] Batch 0/1875                       Loss D: 0.0000, loss G: 19.1621\n",
      "Epoch [12/50] Batch 0/1875                       Loss D: 0.0000, loss G: 18.5723\n",
      "Epoch [13/50] Batch 0/1875                       Loss D: 0.0000, loss G: 21.3019\n",
      "Epoch [14/50] Batch 0/1875                       Loss D: 0.0000, loss G: 20.4443\n",
      "Epoch [15/50] Batch 0/1875                       Loss D: 0.0000, loss G: 20.6325\n",
      "Epoch [16/50] Batch 0/1875                       Loss D: 0.0000, loss G: 21.1128\n",
      "Epoch [17/50] Batch 0/1875                       Loss D: 0.0000, loss G: 21.4852\n",
      "Epoch [18/50] Batch 0/1875                       Loss D: 0.0000, loss G: 22.1356\n",
      "Epoch [19/50] Batch 0/1875                       Loss D: 0.0000, loss G: 22.2235\n",
      "Epoch [20/50] Batch 0/1875                       Loss D: 0.0000, loss G: 22.6977\n",
      "Epoch [21/50] Batch 0/1875                       Loss D: 0.0000, loss G: 22.9653\n",
      "Epoch [22/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.1925\n",
      "Epoch [23/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.2385\n",
      "Epoch [24/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.5642\n",
      "Epoch [25/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.4353\n",
      "Epoch [26/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.6693\n",
      "Epoch [27/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.7349\n",
      "Epoch [28/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.8336\n",
      "Epoch [29/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.9796\n",
      "Epoch [30/50] Batch 0/1875                       Loss D: 0.0000, loss G: 23.9192\n",
      "Epoch [31/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.0195\n",
      "Epoch [32/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.0892\n",
      "Epoch [33/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.1878\n",
      "Epoch [34/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.2497\n",
      "Epoch [35/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.2706\n",
      "Epoch [36/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.3187\n",
      "Epoch [37/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.1680\n",
      "Epoch [38/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.4906\n",
      "Epoch [39/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.4683\n",
      "Epoch [40/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.4970\n",
      "Epoch [41/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.6370\n",
      "Epoch [42/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.4081\n",
      "Epoch [43/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.6501\n",
      "Epoch [44/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.6018\n",
      "Epoch [45/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.6258\n",
      "Epoch [46/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.6954\n",
      "Epoch [47/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.7166\n",
      "Epoch [48/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.8293\n",
      "Epoch [49/50] Batch 0/1875                       Loss D: 0.0000, loss G: 24.7806\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real,_) in enumerate(loader):\n",
    "        real = real.view(-1,784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "        \n",
    "        # Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size,noise_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real , torch.zeros_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake)/2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph =True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output,torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc164a-2360-4407-b310-e15133cd32b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
